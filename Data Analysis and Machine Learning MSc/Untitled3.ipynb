{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb69b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 187.9203 - val_loss: 168.8217\n",
      "Epoch 2/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 166.9245 - val_loss: 164.2847\n",
      "Epoch 3/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 162.9566 - val_loss: 161.6497\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 160.4722 - val_loss: 159.4939\n",
      "Epoch 5/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 158.9344 - val_loss: 158.4329\n",
      "Epoch 6/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 157.8224 - val_loss: 157.4061\n",
      "Epoch 7/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 156.8652 - val_loss: 156.7211\n",
      "Epoch 8/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 156.0978 - val_loss: 156.1834\n",
      "Epoch 9/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 155.4553 - val_loss: 155.8069\n",
      "Epoch 10/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 154.8918 - val_loss: 155.3295\n",
      "Epoch 11/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 154.4154 - val_loss: 154.8292\n",
      "Epoch 12/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 153.9486 - val_loss: 154.4861\n",
      "Epoch 13/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 153.5480 - val_loss: 154.0167\n",
      "Epoch 14/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 153.1733 - val_loss: 153.7286\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 152.8101 - val_loss: 153.3730\n",
      "Epoch 16/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 152.4859 - val_loss: 153.0655\n",
      "Epoch 17/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 152.1425 - val_loss: 152.9216\n",
      "Epoch 18/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 151.8608 - val_loss: 152.6802\n",
      "Epoch 19/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 151.5808 - val_loss: 152.3276\n",
      "Epoch 20/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 151.3168 - val_loss: 152.2650\n",
      "Epoch 21/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 151.0557 - val_loss: 152.0674\n",
      "Epoch 22/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 150.8264 - val_loss: 151.8987\n",
      "Epoch 23/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 150.6222 - val_loss: 151.6283\n",
      "Epoch 24/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 150.3763 - val_loss: 151.5482\n",
      "Epoch 25/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 150.1958 - val_loss: 151.5101\n",
      "Epoch 26/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 150.0228 - val_loss: 151.2974\n",
      "Epoch 27/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 149.8425 - val_loss: 151.0533\n",
      "Epoch 28/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 149.6831 - val_loss: 150.7766\n",
      "Epoch 29/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 149.4831 - val_loss: 150.9344\n",
      "Epoch 30/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 149.3718 - val_loss: 150.6707\n",
      "Epoch 31/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 149.2076 - val_loss: 150.5374\n",
      "Epoch 32/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 149.0826 - val_loss: 150.3437\n",
      "Epoch 33/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 148.9450 - val_loss: 150.3745\n",
      "Epoch 34/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 148.8338 - val_loss: 150.4076\n",
      "Epoch 35/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 148.6898 - val_loss: 150.1089\n",
      "Epoch 36/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 148.5981 - val_loss: 150.0956\n",
      "Epoch 37/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 148.4516 - val_loss: 150.0323\n",
      "Epoch 38/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 148.3667 - val_loss: 149.8763\n",
      "Epoch 39/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 148.2447 - val_loss: 149.8718\n",
      "Epoch 40/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 148.1475 - val_loss: 149.5582\n",
      "Epoch 41/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 148.0327 - val_loss: 149.5848\n",
      "Epoch 42/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 147.9589 - val_loss: 149.7497\n",
      "Epoch 43/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 147.8593 - val_loss: 149.5240\n",
      "Epoch 44/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 147.7808 - val_loss: 149.4525\n",
      "Epoch 45/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 147.6990 - val_loss: 149.3679\n",
      "Epoch 46/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 147.6264 - val_loss: 149.5159\n",
      "Epoch 47/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 147.5412 - val_loss: 149.4583\n",
      "Epoch 48/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 147.4359 - val_loss: 149.2473\n",
      "Epoch 49/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 147.3857 - val_loss: 149.4827\n",
      "Epoch 50/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 147.3299 - val_loss: 148.9983\n",
      "Epoch 51/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 147.2565 - val_loss: 148.8739\n",
      "Epoch 52/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 147.2034 - val_loss: 148.9983\n",
      "Epoch 53/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 147.1127 - val_loss: 149.0007\n",
      "Epoch 54/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 147.0376 - val_loss: 149.1083\n",
      "Epoch 55/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 146.9863 - val_loss: 149.0060\n",
      "Epoch 56/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 146.9082 - val_loss: 148.9905\n",
      "Epoch 57/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 146.8690 - val_loss: 148.9901\n",
      "Epoch 58/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 146.8171 - val_loss: 148.8918\n",
      "Epoch 59/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 146.7316 - val_loss: 149.1988\n",
      "Epoch 60/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 146.6906 - val_loss: 148.5749\n",
      "Epoch 61/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 146.6128 - val_loss: 149.0179\n",
      "Epoch 62/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 146.5877 - val_loss: 148.6204\n",
      "Epoch 63/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 146.5311 - val_loss: 148.7016\n",
      "Epoch 64/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 146.4970 - val_loss: 148.7047\n",
      "Epoch 65/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 146.4208 - val_loss: 148.9487\n",
      "Epoch 66/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 146.4128 - val_loss: 148.5635\n",
      "Epoch 67/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 146.3411 - val_loss: 148.5175\n",
      "Epoch 68/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 146.2945 - val_loss: 148.5196\n",
      "Epoch 69/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 146.2661 - val_loss: 148.4146\n",
      "Epoch 70/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 146.2179 - val_loss: 148.6300\n",
      "Epoch 71/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 146.1686 - val_loss: 148.4743\n",
      "Epoch 72/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 146.1499 - val_loss: 148.5858\n",
      "Epoch 73/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 146.0717 - val_loss: 148.3937\n",
      "Epoch 74/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 146.0358 - val_loss: 148.3636\n",
      "Epoch 75/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 146.0405 - val_loss: 148.7903\n",
      "Epoch 76/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 145.9472 - val_loss: 148.5146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 145.9350 - val_loss: 148.5657\n",
      "Epoch 78/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 145.8987 - val_loss: 148.3852\n",
      "Epoch 79/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 145.8746 - val_loss: 148.3918\n",
      "Epoch 80/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 145.8381 - val_loss: 148.3509\n",
      "Epoch 81/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 145.7948 - val_loss: 148.3186\n",
      "Epoch 82/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 145.7915 - val_loss: 148.2592\n",
      "Epoch 83/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 145.7301 - val_loss: 148.2926\n",
      "Epoch 84/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 145.7074 - val_loss: 148.1311\n",
      "Epoch 85/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 145.6668 - val_loss: 148.2222\n",
      "Epoch 86/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 145.6263 - val_loss: 148.4161\n",
      "Epoch 87/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 145.6018 - val_loss: 148.3825\n",
      "Epoch 88/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 145.5660 - val_loss: 148.4074\n",
      "Epoch 89/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 145.5485 - val_loss: 148.2998\n",
      "Epoch 90/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 145.5246 - val_loss: 148.5342\n",
      "Epoch 91/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 145.4789 - val_loss: 148.1419\n",
      "Epoch 92/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 145.4616 - val_loss: 148.1962\n",
      "Epoch 93/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 145.4483 - val_loss: 148.3656\n",
      "Epoch 94/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 145.3960 - val_loss: 148.2681\n",
      "Epoch 95/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 145.3816 - val_loss: 148.2850\n",
      "Epoch 96/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 145.3357 - val_loss: 148.1993\n",
      "Epoch 97/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 145.2936 - val_loss: 147.8683\n",
      "Epoch 98/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 145.2904 - val_loss: 147.9211\n",
      "Epoch 99/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 145.2881 - val_loss: 147.9655\n",
      "Epoch 100/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 145.2270 - val_loss: 148.0709\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9f27899b28e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mx_test_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "original_dim = 28 * 28\n",
    "intermediate_dim = 64\n",
    "latent_dim = 2\n",
    "\n",
    "inputs = keras.Input(shape=(original_dim,))\n",
    "h = layers.Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = layers.Dense(latent_dim)(h)\n",
    "z_log_sigma = layers.Dense(latent_dim)(h)\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0, stddev=0.1)\n",
    "    return z_mean + K.exp(z_log_sigma)*epsilon\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_sigma])\n",
    "\n",
    "\n",
    "# Create encoder\n",
    "encoder = keras.Model(inputs,[z_mean, z_log_sigma, z], name='encoder')\n",
    "\n",
    "#Create decoder\n",
    "latent_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation='sigmoid')(x)\n",
    "decoder = keras.Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255\n",
    "x_train = x_train.reshape(len(x_train), np.prod(x_train.shape[1:]))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = keras.Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "reconstruction_loss = keras.losses.binary_crossentropy(inputs, outputs)\n",
    "reconstruction_loss *= original_dim\n",
    "kl_loss = 1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "vae.fit(x_train, x_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_data=(x_test, x_test))\n",
    "\n",
    "\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=256)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe09a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
